\section{Our Approach}
Our initial approach is inspired by similar sound recognition problems.
Such problems include the recognition of voice, music, animals vocalisations
including whale song, calls, and so on.
It is observed that these problems, while similar in nature, differ in practice.
This is mainly due to the nature of the sound being analysed, as well as
domain knowledge of the structure.

For example, the acoustics of human voice is well studied, and patterns are
easily identified today.
Specialised methods for analysing voice recordings exist, including intonation
etc etc \textbf{add more}.
we have a mapping, we know the words, we dont speak bird noises

Music recognition is trivial in the case of identifying pure reproductions,
the main challenge in this area being noise reduction and distortion compensation.
Music can be easily identified using statistical methods to compare pitch
variations along the duration of the recording.
Bird song however contains many variations and transpositions within the same
species which make it difficult to find an archetypal sequence of pitch
variations.\\

Some approaches to these problems are essentially spectrographic image recognition
tasks, where elements common to specific labels are searched for within a target
example spectrogram.
The simplicity and success of such solutions has driven the direction of this
project.

Our approach uses a combination of computer vision and machine learning
techniques to construct a fully automatic recognition system.
Standard image processing methods are used to process spectrograms and extract
sections of song which may be used to identify a particular species, much like
how an orthonologist visually inspects the song spectra.
These sections are then matched against new samples to be classified through a
multi-class machine learning algorithm.

move bulk of subsections to appendices

\subsection{Process Overview}
The project is divided into four descrete parts.
These follow the logical flow of data:
\begin{enumerate}
  \item \textbf{Collection:}
    Data is sourced from field recordings done in uncontrolled environments.
    The variety provides a good estimate of real-world performance and
    introduces many quality related issues.

  \item \textbf{Preparation and selection:}
    Recordings are filtered and selected to maintain reasonable quality levels.
    Spectrograms are then derived from the recordings.
    This is now the representation that is used throughout the program until
    the feature vector is constructed.

  \item \textbf{Preprocessing and feature extraction:}
    Noise is reduced as much as possible to identify key regions of interest
    within the spectrogram image.
    These are extracted as templates and cross-correlated against other
    recording spectrograms to form a feature vector.

  \item \textbf{Classification and evaluation:}
    The resulting data is then fed to a classifier and evaluated using techniques
    designed to reduce statistical bias.
\end{enumerate}

Each of these procedures are described in detail in their respective sections.
A few of these sections discuss possible alternatives or improvements to the
developed mechanisms.

A regular focus of the project is performance statistics, which is done for each
stage of the program to some granularity.
A report on this is available in appendix xyz.

\subsection{Architecture Overview}
\textbf{diagram of key sections}

Due to the experimental nature of the project, a rigorous architecture has not
been designed ahead of time.
The codebase evolved through several iterations 


\subsection{Implementation Technologies}
All code is written in Python 2.7.
Libraries used include:
\begin{itemize}
  \item open-CV 2.0
\end{itemize}
