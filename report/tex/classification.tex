\section{Classification}
With feature vectors constructed, we select a machine learning algorithm to
train and classify birdsong using these, giving rise to our model.
This section details the choice of algorithm, it's variations, and performance.
Parameter tuning is discussed is Section~\ref{sec:tuning}

\subsection{Approaches to Classification}
We are approaching this as a multi-class machine learning task, that is, the
classification of a sample as a single label given a set of possibile labels.
A mutli-label approach may also be taken, to identify multiple labels from a
given sample, although this is potentially more complex and has not been
pursued given that most recordings appear to have a single dominant vocalising
bird, and any other sources are notably in the background.

There exist many acceptable multi-class machine learning algorithms.
These may have a discrete or probabilistic output, which is useful to gauge the
confidence level of the classifier.
A multi-class algorithm may also be constructed from an ensemble of several
probabilistic binary classifiers, the final classification of which is computed
by extracting the maximum score of each.
This allows for more algorithms to be tested.

The initial decision of which classifier to use usually depends on the nature
of the data to be classified.
In this case, we are working with a small sample set, but with a very high number
of features.
Samples are labeled, therefore a supervised learning algorithm is appropriate.
Good computational performance is desireable considering our high feature
dimensionality.
In addition, it is very useful to be able to efficiently analyse the impact of
each feature, which is in this case directly measurable as feature importance.

Once an initial algorithm has been chosen, it may be compared to others through
multi-run cross-validation of their accuracies
(ref Choosing between two learning algorithmsbased on calibrated testsRemco R. Bouckaert).\\

The random forest algorithm is well suited for the given task.
It is relatively performant, is not particularly sensitive to bias or noise,
and it is possible to extract individual feature importances directly without
having to develop computationally expensive mechanisms. (ref)
Random forests also exhibit good scalability because it is an ensamble of
independent trees. (ref)

It does not allow for online learning however since trees have to be recomputed, 
and so the addition of extra training samples, features and thus classes can
become expensive for larger additions.
This is somewhat countered by the short time required to train an entire forest.


\subsection{Random Forest Classifier}
A random forest classifier is an ensamble of trees.
A node of a tree is split on an index of the feature vector.
Each Tree uses a new random sampling of the features.

we pick the random forest.
explain why we use this classifier
explain how it fits with the data we are working with

This is a multi-class classifier, returning the probabilities of each class
corresponding to the data given.
An alternative approach is possible, in which the classification of each species
is split into individual random forests, the final result of which is retrieved
by taking the maximum of all probabilities.
Using multiple binary classifiers allows other accuracy metrics to be used,
facilitating the accuracy evaluation of single species.

\subsubsection{Extremely Randomised Trees}
Extremely Randomised Trees is similar to a traditional Random Forest, however
however splitting is randomised, instead of being computed for optimal
performance.
This has the benefit of being faster, with the drawback of being more sensitive
to noisy features.\\

Using this classifier has seen no major accuracy or performance differences.
why do we use it, no reason??

ert thresholds are random, best one is picked, reduces variance, increases bias

\subsection{Parameter Selection}
show parameters available for RF
show initial parameters chosen and their performance
how did we pick these? (make something up)

Section~\ref{sec:tuning} explores semi-automatic parameter tuning and touches on
the issues of overfitting.
