\section{Classification}
With feature vectors constructed, we select a machine learning algorithm to
train and classify birdsong using these, giving rise to our model.
This section discusses the considerations made when choosing an algorithm,
and details our choice of algorithm and variations in its mechanism.
Parameter tuning is discussed is Section~\ref{sec:tuning}

\subsection{Approaches to Classification}\label{sec:clfapp}
We are approaching this as a multi-class machine learning task, that is, the
classification of a sample as a single label given a set of possibile labels.
A mutli-label approach may also be taken, to identify multiple labels from a
given sample, although this potentially is raises the complexity of the problem
somewhat, especially during feature extraction.
Given that most recordings appear to have a single dominant vocalising
bird, this has not been pursued.

There exist many suitable multi-class machine learning algorithms.
It is desireable to obtain a probabilistic output, which gives the probabilities
for each label.
This is useful to gauge the confidence level of the classifier and increase the
transparency of our classifications.

A multi-class classifier may also be constructed from an ensemble of several
binary classifiers by combining the results of each.
Doing so may allow us to use a wider variety of verification metrics and to
identify label-specific behaviours.\\

The initial decision of which classifier to use depends mostly on the nature
of the data being classified.
In our case, we are working with a small sample set, but with a very high 
dimensionality.
Samples are labeled, therefore a supervised learning algorithm is appropriate.
Good computational performance is desireable considering the high feature count.
In addition, it is useful to have an efficient means of analysing the impact of
individual templates, in our case measurable directly through feature importance.

Once an initial algorithm has been chosen, it may be compared to others through
multi-run cross-validation, comparing accuracy results.
\parencite{Bouckaert2003}.

\subsection{Overview of the Random Forest Classifier}
A random forest \parencite{breiman2001} is an ensemble of decision tree classifiers.
Each tree is developed using randomly sampled training data.
Because decision trees are capable of multi-class classification,
forests are also support this natively.

The random forest algorithm is well suited for the given task:
It is relatively performant, not particularly sensitive to bias or noise
(\textcite{marko2004}),
and it is possible to extract individual feature importances directly without
having to develop computationally expensive mechanisms.
Random forests also exhibit good scalability because the algorithm can be easily
parallelised \parencite{breiman2001}.

It does not allow for online learning however since trees have to be recomputed, 
and so the addition of extra training samples, features and thus labels may
become expensive for larger additions.
This is countered somewhat by the short time required to train an entire forest
on our data.

Although we have not done so, it is possible to employ an ensemble of binary
random forest classifiers with probabilistic output.
In such configuration each forest is responsible for classifying a single
species, while considering only the features of that species.
This allows for less expensive updates, as we can add and modify species without
needing to rebuild all of the forests.
Although this method ignores higher-order interactions between class features,
it has been shown to exhibit good performance \parencite{lasseck2013}.\\

We have chosen to use a single multithreaded multi-class random forest.


\subsubsection{Extremely Randomised Trees}
Extremely Randomised Trees \parencite{geurt2006} is an algorithm similar to a
traditional Random Forest, however the thresholds during splitting arerandomised,
instead of being computed for optimal performance.
This has the benefit of being faster, with the drawback of being more sensitive
to noisy features.\\

Using this classifier shows no major accuracy or performance differences.
A speed increase is likely to be observed with many more trees.

\subsection{Parameter Selection}\label{sec:param}
The most impactful parameters for random forest performance are the number of
trees and the number of features at each split.
Random forests are generally trained so that trees are fully grown to minimise
bias, and the number of trees is increased as much as necessary to minimise
variance \parencite{breiman2001}.
Altering the remaining parameters mostly results is tree pruning, which may be
desireable if overfitting occurs.

\begin{itemize}
  \item \textbf{Max features:}
    Defines the maximum number of features that a tree may use at each split.
    Increasing this value generally increases performance at the cost of
    diversity.
    Decreasing this value reduces variance at the increase of bias in individual
    trees.
    The ratio of good features to noise affects the choice of this parameter,
    the less features are selectable, the less chance of picking a good feature
    amongst the set.

  \item \textbf{Number of estimators:}
    Defines the maximum number of trees to build.
    Higher values are generally always better, being detrimental only to speed.
    Increasing the number of trees decreases variance, becoming less
    correlated as features are randomly selected.

    A random forest will eventually converge with the increase in tree count,
    from which the gains begin to diminish (\textcite{breiman2001}).

  \item \textbf{Min samples per leaf node:}
    Defines the size of the leaf nodes.
    Smaller sizes leads to an increase in sensitivity to noise.
    Directly affects the depth of the trees.

  \item \textbf{Max depth:}
    Defines the maximum depth at which a tree may be built.
    If this value is set to |none| with the minimum samples to split set to 1,
    then trees will always be fully developed.

  \item \textbf{Min samples split:}
    Defines the minimum number of sample observations per node required before
    splitting on a feature.
\end{itemize}

The following parameters are set by default and used to gauge the effectiveness
of tuning:
\begin{itemize}[noitemsep]
  \item Estimators: 10
  \item Max features: $sqrt(n)$
  \item Min samples to split: 2
  \item Min samples leaf: 1
  \item Max depth: |none|
\end{itemize}
Max features is a function of the number of features.

For comparison, the following parameters have found to be optimal through the tuning mechanism
discussed in Section~\ref{sec:tuning}, and is used throughout evaluation in
Sections~\ref{sec:acc_eval} and \ref{sec:feature_imp}:
\begin{itemize}[noitemsep]
  \item Estimators: ?
  \item Max features: ?
  \item Min samples to split: ?
  \item Min samples leaf: ?
  \item Max depth: ?
\end{itemize}
