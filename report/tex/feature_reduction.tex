\section{Feature Importance}\label{sec:feature_imp}

FEATURE IMPORTANCES ARE COMPUTED DURING TRAINING! SEE OPTIMIZATIONS

Because our feature vector is directly correlated with the templates of each species,
it is possible to determine their impact and quality by computing feature
importances from our classifier.
This allows us to reduce the number of templates involved in classifying a given
sample, significantly cutting down the cost of template matching per species.
Feature importances also help us analyse the preprocessing performance in detail,
as well as correlate inter-species similarities in cases where there is a high
level of confusion.

\subsection{In Random Forests}

The two most common methods for retrieving feature importance from a random
are detailed in this section.

\subsubsection{Mean decrease accuracy}
One method for measuring the impact of each feature is through the mean decrease
in accuracy.
With this method, the impact of removing each feature one-by-one is measured.
This method may be sensitive to the random nature of the classifier, so multiple
runs may be required to eliminate any variance (is this true?).
This can be prohibitively expensive.

\subsubsection{Mean decrease impurity}
Because the nodes of trees in a random forest correlate directly with a specific
feature, it is possible to directly measure or estimate the importance of each
feature by determining the probability of a node in a tree being traversed over 
the number of nodes in the forest.
This is known as the mean decrease impurity, or gini importance, which is what
is used in Sckikit's random forest classifier implementation.

\subsection{Results and Analysis}
Measuring feature importances exposes 50\% (x out of y) templates as 
completely irrelevant for classification within our dataset
(Figure~\ref{fig:fimp}).
Removing these features shows no reduction in accuracy, and a significant speedup
during template matching is gained, with an estimated reduction of x minutes per
spectrogram, from y to z with an average of j per template given average
dimensions as shown in Section~\ref{sec:ccm}.

There exist 35773 templates distributed across 20 spectrograms for each
of the 12 species selected.
When classifying a new sample, it's spectrogram must be cross-correlated with
each of the templates.
This operation is extremely expensive, taking up to 70 hours, or approximately
5.8 hours per sample, 0.6 seconds per template on average.

The aim is therefore to reduce the feature count as much as possible while
retaining the highest possible accuracy by determining an appropriate importance
cutoff.
Removing features and reevaluating the classifier takes no significant time in
contrast to template extraction and may therefore be done by iteratively removing
and checking classifier accuracy, adjusting the importance cutoff accordingly.
Measuring feature importance unfortunately requires a complete run which involves
the cross-correlation of all templates, which means any performance benefits will
only affect new sample classifications.\\

Feature reduction is however beneficial during feature extraction, when merging
the results of two batches.
Templates with importance scores equalling or close to zero can be safely removed
from the set before merging with another batch, saving a considerable amount of time.
Although we have not done this in practice the estimated amount of time saved is
in the order of x hours assuming a consistent 50\% template reduction rate.

\begin{figure}[!htb]
  \centering
  \caption{Visualisation of feature importances}
  \label{fig:fimp}
\end{figure}


\textbf{paragraph on important and non important feature analysis. lots of 
images, look at templates belonging to confused species, try to determine
which are culprits, probably most of them}
